{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ada0643c87b745dd976ab0f0174074e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_052c957283914665b48a6387fc885a6a","IPY_MODEL_0b2a544ed0e7446883b82fd76a183883","IPY_MODEL_1f2a4e7e35c343c394078290040f6899"],"layout":"IPY_MODEL_3afde954f71c4ed0a318af3e8bf9f355"}},"052c957283914665b48a6387fc885a6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bebd9b8a4d914c6f96da44513350485f","placeholder":"â€‹","style":"IPY_MODEL_11f5311dcd394434a61f8169c5953fde","value":"Map:â€‡100%"}},"0b2a544ed0e7446883b82fd76a183883":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ea493146b654708be74e420b72db2ef","max":23552,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef92622717564edfb7745cf4ee42a234","value":23552}},"1f2a4e7e35c343c394078290040f6899":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34aa328f0d814e2e9b33cc44f14623f4","placeholder":"â€‹","style":"IPY_MODEL_f4369aeb6bc24c6cbe5b1de035cf1964","value":"â€‡23552/23552â€‡[00:02&lt;00:00,â€‡6212.29â€‡examples/s]"}},"3afde954f71c4ed0a318af3e8bf9f355":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bebd9b8a4d914c6f96da44513350485f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11f5311dcd394434a61f8169c5953fde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ea493146b654708be74e420b72db2ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef92622717564edfb7745cf4ee42a234":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34aa328f0d814e2e9b33cc44f14623f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4369aeb6bc24c6cbe5b1de035cf1964":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b168d8a6dc84cf6889ce6d21d783deb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd0e644eee1c442a8d83b22ed19af843","IPY_MODEL_5281e995bce141c0b3de775b484666cc","IPY_MODEL_842a6798092e43579bea62187cc0fd26"],"layout":"IPY_MODEL_31f795b25b1441b5a2421d5c79f9ec7e"}},"bd0e644eee1c442a8d83b22ed19af843":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ff815b06ea84ecfa6fc3aefbd783af2","placeholder":"â€‹","style":"IPY_MODEL_c2bab47b17d747a2948340fb855d40e5","value":"Map:â€‡100%"}},"5281e995bce141c0b3de775b484666cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26004d6e2e13494aac26a4e7eb98575e","max":23552,"min":0,"orientation":"horizontal","style":"IPY_MODEL_964011d7468447cba228f5e13850fb3e","value":23552}},"842a6798092e43579bea62187cc0fd26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20343efdf18d47639f315c3fcad0d7c3","placeholder":"â€‹","style":"IPY_MODEL_be4c9aa86b324069ad33da8e250fc0ca","value":"â€‡23552/23552â€‡[00:13&lt;00:00,â€‡1854.29â€‡examples/s]"}},"31f795b25b1441b5a2421d5c79f9ec7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ff815b06ea84ecfa6fc3aefbd783af2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2bab47b17d747a2948340fb855d40e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26004d6e2e13494aac26a4e7eb98575e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"964011d7468447cba228f5e13850fb3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20343efdf18d47639f315c3fcad0d7c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be4c9aa86b324069ad33da8e250fc0ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Fine Tuning using *xlm-roberta-base*"],"metadata":{"id":"St_H2j7d0P1B"}},{"cell_type":"markdown","source":["## Installing the `datasets` Library\n","\n","In order to fine-tune our LLM, we need to manage and preprocess large datasets efficiently. The `datasets` library by Hugging Face provides tools to load, manipulate, and share datasets for NLP tasks, including Named Entity Recognition (NER)."],"metadata":{"id":"FQaLyzAX1cah"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzWGjyJ1XtAh","executionInfo":{"status":"ok","timestamp":1727794737910,"user_tz":-180,"elapsed":9081,"user":{"displayName":"zembol yechala","userId":"00101992037674450041"}},"outputId":"eaae6840-e347-472b-cbad-edd2319fcbea","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"]}],"source":["!pip install datasets"]},{"cell_type":"markdown","source":["## Loading CoNLL-formatted NER Dataset\n","\n","The function `load_conll_dataset` is designed to load and parse data in **CoNLL format** for **Named Entity Recognition (NER)** tasks. This format is commonly used for NER, where each line contains a word and its corresponding entity label, and sentences are separated by blank lines.\n","\n","### Breakdown of the Function:\n","\n","1. **Parsing CoNLL Data**:  \n","   The inner function `parse_conll(file_path)` reads the CoNLL file line by line:\n","   - Each line contains a **word** and its **label**, separated by a space.\n","   - Sentences are stored as lists of word-label pairs, and blank lines signify the end of a sentence.\n","   - After processing all lines, the sentences are grouped in a list, where each sentence is a list of tuples, with each tuple containing a word and its corresponding label.\n","\n","2. **Data Formatting**:  \n","   The parsed data is converted into a dictionary:\n","   - **\"tokens\"**: A list of lists, where each inner list contains the tokens (words) of a sentence.\n","   - **\"ner_tags\"**: A list of lists, where each inner list contains the NER labels corresponding to the tokens in the same sentence.\n","\n","3. **Dataset Creation**:  \n","   The dictionary is then passed to Hugging Faceâ€™s `Dataset.from_dict()` method, which converts the parsed data into a Hugging Face `Dataset` object. This allows us to utilize the powerful tools provided by the Hugging Face ecosystem for further processing, training, and evaluation of the model.\n"],"metadata":{"id":"z1Ts3VCl1v_v"}},{"cell_type":"code","source":["import pandas as pd\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","\n","def load_conll_dataset(file_path):\n","    # Function to parse CoNLL data and return it as a Hugging Face Dataset\n","    def parse_conll(file_path):\n","        sentences = []\n","        current_sentence = []\n","\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                line = line.strip()\n","                if line == \"\":  # New sentence\n","                    if current_sentence:\n","                        sentences.append(current_sentence)\n","                        current_sentence = []\n","                else:\n","                    word, label = line.split()  # Assumes word and label are separated by space\n","                    current_sentence.append((word, label))\n","\n","            if current_sentence:  # Catch any remaining sentence\n","                sentences.append(current_sentence)\n","\n","        return sentences\n","\n","    # Parse the data\n","    parsed_sentences = parse_conll(file_path)\n","\n","    # Prepare the data in dictionary format\n","    data = {\n","        \"tokens\": [[word for word, label in sentence] for sentence in parsed_sentences],\n","        \"ner_tags\": [[label for word, label in sentence] for sentence in parsed_sentences],\n","    }\n","\n","    # Create and return a Hugging Face dataset\n","    dataset = Dataset.from_dict(data)\n","\n","    return dataset\n","\n","#Load the dataset\n","file_path = \"/content/labeled_data_merged_file.conll\"\n","dataset = load_conll_dataset(file_path)\n"],"metadata":{"id":"uKr1Mv2SZGo5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SpQq2c7giRQt","executionInfo":{"status":"ok","timestamp":1727795166129,"user_tz":-180,"elapsed":541,"user":{"displayName":"zembol yechala","userId":"00101992037674450041"}},"outputId":"6ddd8867-677c-4d09-8be8-19f81a991249"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['tokens', 'ner_tags'],\n","    num_rows: 23552\n","})"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["# Tokenizer Initialization\n","\n","In this section, we initialize the tokenizer using a pre-trained model from Hugging Face. The **tokenizer** is responsible for converting raw text (sentences or tokens) into input tokens that the model can process during training or inference. For this project, we can select a model that supports Amharic language tokenization. we're currently using the `distilbert-base-uncased` model."],"metadata":{"id":"hv-nmb-E2Uy2"}},{"cell_type":"code","source":["#Initialize the tokenizer (using a model that supports Amharic, like XLM-R or bert-tiny-amharic)\n","# model_name = \"distilbert-base-uncased\"\n","# model_name = \"rasyosef/bert-tiny-amharic\"\n","# model_name = \"bert-base-multilingual-cased\"\n","model_name = \"xlm-roberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"dfDPZZPaZ-cp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Encoding NER Labels\n","\n","The `encode_labels` function is responsible for converting human-readable Named Entity Recognition (NER) tags (such as `B-Product`, `I-Product`, etc.) into integer-encoded labels. This is necessary because machine learning models typically require labels in numerical form."],"metadata":{"id":"6IgPueEs422j"}},{"cell_type":"code","source":["def encode_labels(data):\n","  # NER tags\n","  ner_tags = ['B-Product', 'I-Product', 'B-PRICE','I-PRICE', 'B-LOC', 'I-LOC','O']\n","\n","  # Create a dictionary to map each tag to a unique integer\n","  label_to_id = {label: idx for idx, label in enumerate(ner_tags)}\n","  data['ner_tags'] = [label_to_id[tag] for tag in data['ner_tags']]\n","\n","  # # Get unique NER labels (used in evaluation)\n","  # label_list = sorted(label_to_id.keys())\n","\n","  return data\n","\n","# Apply the function to the dataset\n","encoded_dataset = dataset.map(encode_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["ada0643c87b745dd976ab0f0174074e7","052c957283914665b48a6387fc885a6a","0b2a544ed0e7446883b82fd76a183883","1f2a4e7e35c343c394078290040f6899","3afde954f71c4ed0a318af3e8bf9f355","bebd9b8a4d914c6f96da44513350485f","11f5311dcd394434a61f8169c5953fde","5ea493146b654708be74e420b72db2ef","ef92622717564edfb7745cf4ee42a234","34aa328f0d814e2e9b33cc44f14623f4","f4369aeb6bc24c6cbe5b1de035cf1964"]},"id":"oi0JwOPzbTAI","executionInfo":{"status":"ok","timestamp":1727795191680,"user_tz":-180,"elapsed":3486,"user":{"displayName":"zembol yechala","userId":"00101992037674450041"}},"outputId":"47be8d32-1992-42b2-b758-083c4b290ec9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/23552 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ada0643c87b745dd976ab0f0174074e7"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Tokenizing and Aligning Labels\n","\n","The `tokenize_and_align_labels` function is used to tokenize input sentences while ensuring that the NER labels (or any other token-level labels) remain aligned with the tokens after subword tokenization. This is essential when working with models like BERT or DistilBERT, which tokenize words into subwords, as we need to make sure that the NER tags still map correctly to their corresponding words or subwords.\n","\n","In this step, we use the `tokenize_and_align_labels` function to tokenize the dataset and ensure that the NER tags (which are now encoded as integers) remain aligned with the tokens after subword tokenization.\n"],"metadata":{"id":"5mbI2z4J5Spi"}},{"cell_type":"code","source":["def tokenize_and_align_labels(dataset, tokenizer, label_all_tokens=False):\n","    def tokenize_and_align(examples):\n","        tokenized_inputs = tokenizer(examples['tokens'], truncation=True,padding=True, is_split_into_words=True)\n","\n","        labels = []\n","        for i, label in enumerate(examples[\"ner_tags\"]):\n","            word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n","            label_ids = []\n","            previous_word_idx = None\n","            for word_idx in word_ids:\n","                if word_idx is None:\n","                    label_ids.append(-100)  # Special tokens\n","                elif word_idx != previous_word_idx:  # Only label first subword\n","                    label_ids.append(label[word_idx])\n","                else:\n","                    label_ids.append(-100)\n","                previous_word_idx = word_idx\n","            labels.append(label_ids)\n","\n","        tokenized_inputs[\"labels\"] = labels\n","        return tokenized_inputs\n","\n","    tokenized_dataset = dataset.map(tokenize_and_align, batched=True)\n","    return tokenized_dataset\n","#Tokenize and align labels\n","tokenized_dataset = tokenize_and_align_labels(encoded_dataset, tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["2b168d8a6dc84cf6889ce6d21d783deb","bd0e644eee1c442a8d83b22ed19af843","5281e995bce141c0b3de775b484666cc","842a6798092e43579bea62187cc0fd26","31f795b25b1441b5a2421d5c79f9ec7e","4ff815b06ea84ecfa6fc3aefbd783af2","c2bab47b17d747a2948340fb855d40e5","26004d6e2e13494aac26a4e7eb98575e","964011d7468447cba228f5e13850fb3e","20343efdf18d47639f315c3fcad0d7c3","be4c9aa86b324069ad33da8e250fc0ca"]},"id":"MwdPOxIaaSip","executionInfo":{"status":"ok","timestamp":1727795211025,"user_tz":-180,"elapsed":13978,"user":{"displayName":"zembol yechala","userId":"00101992037674450041"}},"outputId":"b7644372-3d10-49c4-a55e-6c31e3aa8120"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/23552 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b168d8a6dc84cf6889ce6d21d783deb"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Splitting the Dataset into Training and Validation Sets\n","\n","Once the dataset has been tokenized and the labels have been aligned, the next step is to split it into **training** and **validation** sets. This allows us to train the model on one portion of the data and validate its performance on another portion, which helps prevent overfitting."],"metadata":{"id":"9Ho8hX0W8vH1"}},{"cell_type":"code","source":["split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n","train_dataset = split_dataset[\"train\"]\n","val_dataset = split_dataset[\"test\"]"],"metadata":{"id":"LmHtdTMCchZg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setting Up Training Arguments for Fine-Tuning\n","\n","The `TrainingArguments` class from Hugging Face's `transformers` library provides an easy way to configure training hyperparameters for fine-tuning transformer models. In this section, we define key arguments that control the training process, such as the evaluation strategy, learning rate, batch size, number of epochs, and weight decay."],"metadata":{"id":"fk-yeD9K9c3D"}},{"cell_type":"code","source":["#Setting up training arguments\n","from transformers import TrainingArguments\n","\n","def setup_training_args(output_dir):\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n","        learning_rate=2e-5,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","    )\n","    return training_args\n","# Set up training arguments\n","output_dir = \"./fine_tuned_model\"\n","training_args = setup_training_args(output_dir)"],"metadata":{"id":"vLuRA7hOd5gK","executionInfo":{"status":"ok","timestamp":1727795226310,"user_tz":-180,"elapsed":591,"user":{"displayName":"zembol yechala","userId":"00101992037674450041"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"63e3d125-66b2-48cb-af06-9bbe4ef81d00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## Custom Evaluation Function for Model Metrics\n","\n","To assess the performance of our fine-tuned Named Entity Recognition (NER) model, we define a custom evaluation function `compute_metrics`. This function calculates various performance metrics such as accuracy, precision, recall, and F1 score, which are essential for understanding the model's effectiveness."],"metadata":{"id":"JjoNbLXI-CHx"}},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","import numpy as np\n","\n","# Custom evaluation function to calculate accuracy, precision, recall, and F1\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (usually padding tokens) from the labels\n","    true_labels = [[label for (pred, label) in zip(prediction, label) if label != -100] for prediction, label in zip(predictions, labels)]\n","    true_predictions = [[pred for (pred, label) in zip(prediction, label) if label != -100] for prediction, label in zip(predictions, labels)]\n","\n","    # Flatten lists for metric calculation\n","    true_labels_flat = [label for sublist in true_labels for label in sublist]\n","    true_predictions_flat = [pred for sublist in true_predictions for pred in sublist]\n","\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, true_predictions_flat, average=\"weighted\")\n","    accuracy = accuracy_score(true_labels_flat, true_predictions_flat)\n","\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1,\n","    }\n"],"metadata":{"id":"vbL1eocnkG_t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-Tuning the Model for Named Entity Recognition (NER)\n","\n","Fine-tuning a pre-trained transformer model on a specific task,  Named Entity Recognition (NER) in our case, is crucial for adapting the model to recognize relevant entities within your dataset. In this section, we set up and execute the fine-tuning process using Hugging Faceâ€™s `Trainer`."],"metadata":{"id":"HwD_5-7a-SLD"}},{"cell_type":"code","source":["#  Fine-tune the model\n","from transformers import AutoModelForTokenClassification, Trainer,  DataCollatorForTokenClassification\n","\n","def fine_tune_model(model_name, train_dataset, val_dataset, training_args, tokenizer):\n","    # Load pre-trained model\n","    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n","\n","    # Initialize the DataCollator with padding and truncation\n","    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n","\n","    # Initialize the Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics,\n","        data_collator=data_collator\n","    )\n","\n","    # Fine-tune the model\n","    trainer.train()\n","\n","    return model,trainer\n","\n","\n","num_labels = 7 # Since we have seven tags\n","model,trainer = fine_tune_model(model_name, train_dataset, val_dataset, training_args, tokenizer)\n"],"metadata":{"id":"V04T1PpTeG_K","executionInfo":{"status":"ok","timestamp":1727801850795,"user_tz":-180,"elapsed":6499873,"user":{"displayName":"zembol yechala","userId":"00101992037674450041"}},"colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"fc74a1e7-09b7-41cb-b912-5eca553658bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3534' max='3534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3534/3534 1:48:14, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.088700</td>\n","      <td>0.058463</td>\n","      <td>0.982310</td>\n","      <td>0.982217</td>\n","      <td>0.982310</td>\n","      <td>0.982248</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.046600</td>\n","      <td>0.038253</td>\n","      <td>0.988866</td>\n","      <td>0.988855</td>\n","      <td>0.988866</td>\n","      <td>0.988851</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.025700</td>\n","      <td>0.034308</td>\n","      <td>0.990381</td>\n","      <td>0.990436</td>\n","      <td>0.990381</td>\n","      <td>0.990403</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Evaluation\n","Calculates various performance metrics such as accuracy, precision, recall, and F1 score"],"metadata":{"id":"sbFzkyzvdeRq"}},{"cell_type":"code","source":["# Evaluate the model\n","evaluation_results = trainer.evaluate()\n","print(evaluation_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"r3bjkd8dluYd","executionInfo":{"status":"ok","timestamp":1727802585665,"user_tz":-180,"elapsed":146156,"user":{"displayName":"zembol yechala","userId":"00101992037674450041"}},"outputId":"9de47754-1c35-4907-dd02-4f171a4de829"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [295/295 02:23]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.0343075767159462, 'eval_accuracy': 0.9903814990224675, 'eval_precision': 0.9904359699629325, 'eval_recall': 0.9903814990224675, 'eval_f1': 0.9904030810200298, 'eval_runtime': 145.678, 'eval_samples_per_second': 32.338, 'eval_steps_per_second': 2.025, 'epoch': 3.0}\n"]}]},{"cell_type":"markdown","source":["## Save Model and Tokenizer\n","Save model and tokenizer for later use"],"metadata":{"id":"gsLwAqSPd2NG"}},{"cell_type":"code","source":["# Save the fine-tuned model\n","def save_model(model, tokenizer,output_dir):\n","    model.save_pretrained(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","\n","\n","save_model(model,tokenizer,output_dir)"],"metadata":{"id":"lCnS-PAwdgtv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForTokenClassification, AutoTokenizer\n","import torch\n","\n","# Load the fine-tuned model and tokenizer\n","model = AutoModelForTokenClassification.from_pretrained(\"./fine_tuned_model\")\n","tokenizer = AutoTokenizer.from_pretrained(model_name)  # Ensure you use the same tokenizer\n","\n","def predict_ner(sentence):\n","    # Step 1: Tokenize the input sentence\n","    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, is_split_into_words=False)\n","\n","    # Step 2: Make predictions\n","    with torch.no_grad():  # Disable gradient calculation\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","\n","    # Get the predicted label indices\n","    predictions = torch.argmax(logits, dim=2)\n","\n","    # Step 3: Map predictions back to labels\n","    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","    predicted_labels = []\n","    for i, token in enumerate(tokens):\n","        if token in tokenizer.special_tokens_map.values():  # Skip special tokens\n","            predicted_labels.append(-1)  # Assign a default value for special tokens\n","        else:\n","            # Append the prediction for the token (convert from tensor to int)\n","            predicted_labels.append(predictions[0][i].item())\n","\n","    # Map the predicted indices to NER labels\n","    ner_tags = ['B-Product', 'I-Product', 'B-PRICE', 'I-PRICE', 'B-LOC', 'I-LOC', 'O']\n","\n","    # Convert predicted label indices to string labels (use the correct mapping here)\n","    predicted_labels = [ner_tags[label] if label != -1 else \"O\" for label in predicted_labels]\n","\n","    return tokens, predicted_labels\n","\n","# Example usage\n","sentence = \"áŠ á‹­ááŠ• 15 á•áˆ® áˆ›áŠ­áˆµ á‹‹áŒ‹ 10000 á‰¥áˆ­ áŠ á‹µáˆ­áˆ» áŠ á‹³áˆ›\"\n","tokens, predicted_labels = predict_ner(sentence)\n","\n","# Display the tokens and their predicted labels\n","for token, label in zip(tokens, predicted_labels):\n","    print(f\"{token}: {label}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FmtBEBHbEps1","executionInfo":{"status":"ok","timestamp":1727787361453,"user_tz":-180,"elapsed":3016,"user":{"displayName":"wolderufael kassahun","userId":"15197202352485798255"}},"outputId":"c828f954-29bf-48f6-a0c8-7638f319ceed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s>: O\n","â–áŠ á‹­: I-Product\n","á: I-Product\n","áŠ•: I-Product\n","â–15: I-Product\n","â–: I-Product\n","á•áˆ®: I-Product\n","â–áˆ›: I-Product\n","áŠ­áˆµ: I-Product\n","â–á‹‹áŒ‹: B-PRICE\n","â–10000: I-PRICE\n","â–á‰¥áˆ­: I-PRICE\n","â–áŠ á‹µáˆ­: I-PRICE\n","áˆ»: O\n","â–áŠ á‹³: O\n","áˆ›: O\n","</s>: O\n"]}]}]}